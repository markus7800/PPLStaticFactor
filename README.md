# Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops

Overview:
```
. 
├── evaluation/
│     ├── benchmark/                        # benchmark set of 16 probabilistic models
│     │     └── generated/                  # sub-programs generated by generate_factorisation.py script
│     ├── custom/                           # implementation of custom factorisations (GMM + LDA)
│     ├── finite/                           # implementation of fixed-finite factorisation
│     ├── unrolled/                         # programs where while loop can be syntactically unrolled
│     │     └── generated/                  # sub-programs generated by generate_factorisation.py script
│     ├── bench.jl                          # script to run benchmark for model
│     ├── paper_lmh_results.csv             # runtime results reported in paper
│     └── ppl.jl                            # implemenation of research PPL
│
├── compare/
│     ├── gen/                              # benchmark set of 16 probabilistic models re-implemented in Gen.jl
│     │     ├── run_lmh.py                  # script to run combinator inference for every model
│     │     ├── paper_gen_results.csv       # Gen runtime results reported in paper
│     │     └── geometric_recurse.jl        # Demonstrating why we could not evaluate Recurse combinator
│     └── webppl/                           # benchmark set of 16 probabilistic models re-implemented in WebPPL
│           ├── run_lmh.py                  # script to run C3 inference for every model
│           └── paper_webppl_results.csv    # WebPPL C3 runtime results reported in paper
│
├── src/   
│     ├── formal/
│     │     ├── ast.jl                      # get abstract syntax tree for julia program
│     │     ├── factorisation_builder.py    # uses model_graph.py to write sub-programs for each factor of probabilistic program
│     │     └── formal_cfg.py               # computes the CFG for probabilistic program
│     └── static/
│           ├── cfg.py                      # types for building CFG
│           ├── ir.py                       # intermediate represenation of probabilstic program
│           ├── model_graph.py              # computes dependencies between sample statements as graph
│           └── rd_bp.py                    # functionality to compute reaching definitions and branch parents
│
├── generate_factorisation.py               # script to generate sub-programs for each factor for every model
└── run_lmh_benchmark.py                    # script to run bench_lmh.jl for every model
```

## Setup

No special hardware is needed for installation.

Recommendations:
- Hardware: >= 3.5 GHZ dual core CPU, >= 8 GB RAM, and >= 10 GB storage
- OS: unix-based operating system
- Installation with Docker

### Docker Installation

Install [docker](https://www.docker.com).

Build the pplstaticfactor image (this may take several minutes):
```
docker build -t pplstaticfactor .
```

If the build was successful, run the docker image:
```
docker run -it --name pplstaticfactor --rm pplstaticfactor
```

### Environment Reference for Custom Installation
- `Python 3.10.12` with `requirements.txt`
  - `sexpdata==1.0.2`
  - `pandas==2.2.3`
- `Julia 1.9.2` with `Project.toml`
  - `Distributions = "0.25.112"`
  - `JuliaSyntax = "0.4.10"`
  - `Gen = "0.4.7"`
- `node.js 23.10`
  - `webppl@0.9.15`


## Replication of Paper

Generate the sub-programs:
```
python3 generate_factorisation.py
```

Run benchmarks `N` times (we set `N` = `10`):
```
python3 run_lmh_benchmark.py N
```
The results are written to `evaluation/lmh_results.csv` and aggregated in `evaluation/lmh_results_aggregated.csv`.  
The results reported in the paper can be found in  `evaluation/paper_lmh_results.csv` (measured on a M2 Pro CPU).

This script runs the `bench_lmh.jl` file which measures the runtime for the factored and non-factored versions of the LMH algorithm and asserts that the samples are the same for all LMH implementations.

For comparison run (we set `N` = `10`):
```
python3 compare/gen/run_lmh.py N   
```
and
```
python3 compare/webppl/run_lmh.py N
```
The results are written to `compare/gen/results.csv` and `compare/webppl/results.csv` and aggregated in `compare/gen/results_aggregated.csv` and `compare/webppl/results_aggregated.csv`, respectively.  
The results reported in the paper can be found in  `compare/gen/paper_results.csv` and `compare/webppl/paper_results.csv` (measured on a M2 Pro CPU).